# LingualLeap â€” Open Language Learning Playground

I originally built this app to practice languages for myself and then decided to open it up so anyone can clone, learn,
and tweak it. Itâ€™s intentionally simple but productionâ€‘minded, with a clean Next.js setup and a few opinionated choices
for DX.

> If this helped you, a â­ on the repo goes a long way!

---

## âœ¨ Whatâ€™s inside

- **Next.js 15 (App Router)** + **TypeScript** + **Tailwind** + **shadcn/ui**
- **AI features (Genkit + Google AI / Gemini)**
    - Conversational **Tutor**
    - **Grammar** check & short explanations
    - **Refinement** / rewriting suggestions
    - **Translations**
- **Textâ€‘toâ€‘Speech (Azure Speech)** endpoint for audio playback
- Minimal, composable components and utilities so you can extend quickly

---

## ğŸš€ Quick start

### 1) Clone and install

```bash
git clone <YOUR_REPO_URL>.git
cd <YOUR_REPO_FOLDER>
# choose your favorite:
pnpm install   # or
npm install    # or
yarn
```

### 2) Create your environment file

Create a `.env.local` in the project root (Next.js loads this automatically).

<details>
<summary>.env.local example</summary>

```dotenv
# --- Azure Speech (Text-to-Speech) ---
AZURE_SPEECH_KEY=your_azure_speech_key
AZURE_SPEECH_REGION=westeurope   # e.g., westeurope, eastus, etc.
AZURE_SPEECH_DEFAULT_VOICE=en-US-JennyNeural  # optional

# --- Google AI / Gemini (pick one of these integration paths) ---
# If using Google AI Studio API key (server-side calls):
GOOGLE_API_KEY=your_google_ai_studio_api_key

# If using Vertex AI via service account, you might rely on ADC (Application Default Credentials):
# GOOGLE_APPLICATION_CREDENTIALS=/absolute/path/to/your-service-account.json
# GCLOUD_PROJECT=your-gcp-project-id
# GOOGLE_CLOUD_PROJECT=your-gcp-project-id

# --- App settings (optional) ---
NEXT_PUBLIC_APP_NAME=LingualLeap
```

</details>

> Notes
> - **Azure Speech:** Create a Speech resource in the Azure portal, copy the **Key** and **Region**, and pick a voice (
    e.g., `en-US-JennyNeural`, `de-DE-KatjaNeural`).
> - **Gemini:** Either use **Google AI Studio** (`GOOGLE_API_KEY`) or **Vertex AI** with service account credentials.
    Configure Genkit to use the provider you prefer.

### 3) Run the app

```bash
pnpm dev    # or npm run dev, or yarn dev
```

Open http://localhost:3000 and youâ€™re in ğŸ‰

---

## ğŸ§  How it works

### High-level architecture

- **UI:** Next.js App Router pages (`/app`) with Tailwind + shadcn/ui components.
- **AI flows (Genkit + Gemini):** Server-side utilities that call Gemini for tutor chat, grammar checks, refinements,
  and translations.
- **Audio:** A **Next.js Route Handler** at `/api/tts` that proxies requests to **Azure Speech** and returns generated
  audio (stream or base64 depending on your implementation).

### TTS API (server)

`POST /api/tts`

**Body (JSON):**

```json
{
  "text": "Hallo! Wie geht's dir?",
  "lang": "de-DE",
  "voice": "de-DE-KatjaNeural",
  "rate": "1.1"
  // optional speed multiplier or SSML rate value
}
```

**Response:** audio data (e.g., `audio/mpeg`). You can keep it simple and return a Buffer/stream or respond with a data
URLâ€”choose what fits your player.

> Tip: If you need per-language defaults, set a small mapping in the route handler (e.g., `de-DE` â†’
`de-DE-KatjaNeural`).

---

## ğŸ—‚ï¸ Suggested directory structure

```
.
â”œâ”€ app/
â”‚  â”œâ”€ (marketing)/â€¦            # optional landing routes
â”‚  â”œâ”€ learn/â€¦                  # main learning experience
â”‚  â”œâ”€ api/
â”‚  â”‚  â””â”€ tts/route.ts          # Azure Speech TTS handler
â”‚  â””â”€ page.tsx                 # home
â”œâ”€ components/                 # UI components (cards, chat, forms, etc.)
â”œâ”€ lib/                        # AI flows, adapters, helpers
â”‚  â”œâ”€ ai/
â”‚  â”‚  â”œâ”€ tutor.ts              # conversational tutor flow
â”‚  â”‚  â”œâ”€ grammar.ts            # grammar check flow
â”‚  â”‚  â”œâ”€ refine.ts             # rewriting/refinement flow
â”‚  â”‚  â””â”€ translate.ts          # translation flow
â”‚  â”œâ”€ tts.ts                   # tiny client util that calls /api/tts
â”‚  â””â”€ types.ts                 # shared types
â”œâ”€ public/                     # images, icons, (optional) audio samples
â”œâ”€ styles/                     # globals.css, tailwind config imports
â”œâ”€ .env.local.example          # sample env (optional)
â”œâ”€ next.config.js(ts)          # Next.js config
â”œâ”€ tailwind.config.ts          # Tailwind setup
â””â”€ README.md
```

---

## ğŸ§ª Scripts

```bash
# Dev
pnpm dev

# Build & run
pnpm build
pnpm start

# Lint & type-check
pnpm lint
pnpm typecheck
```

(Replace `pnpm` with `npm run` or `yarn` if thatâ€™s your preference.)

---

## ğŸ§© Extending the app

- **Add new â€œskillsâ€**: Create a new Genkit flow in `lib/ai/` (e.g., `summarize.ts`) and wire a UI surface for it.
- **Swap providers**: You can replace Gemini with another LLM by introducing a new adapter. Keep the call sites in
  `lib/ai/` to isolate provider concerns.
- **Persist sessions**: Add a database (e.g., Postgres + Prisma) to store chat turns, progress, vocab lists, etc.
- **Auth**: Drop in NextAuth/Auth.js to personalize experiences per user.

---

## ğŸ› Troubleshooting

- **Audio doesnâ€™t play**: Check `AZURE_SPEECH_KEY`, `AZURE_SPEECH_REGION`, and confirm the **voice** supports your
  `lang` (region/locale matters). Inspect Network tab for `/api/tts` response type.
- **AI calls fail**: Confirm `GOOGLE_API_KEY` or Vertex AI credentials. Ensure the server-side code avoids exposing keys
  to the browser.
- **Edge vs Node runtimes**: If you run route handlers on the Edge runtime, some SDKs may not work. Prefer
  `export const runtime = "nodejs"` in route handlers that use native modules/SDKs.

---

## ğŸ›£ï¸ Roadmap (ideas)

- Spacedâ€‘repetition vocabulary drills
- CEFRâ€‘aligned goals & progress tracking
- Inline grammar hints while chatting
- Voice input (STT) & listening exercises
- Multiâ€‘language voice presets and perâ€‘user defaults

---

## ğŸ¤ Contributing

PRs and issues are welcome! If you ship a new flow or language preset, please add docs/snippets so others can benefit.

1. Fork the repo
2. Create a feature branch (`feat/<name>`)
3. Commit with clear messages
4. Open a PR with context and screenshots/video if relevant

---

## ğŸ“œ License

MIT â€” see `LICENSE`. Youâ€™re free to clone, modify, and build on top of this. Attribution appreciated but not required.

---

## ğŸ™Œ Why public?

I built this for my own language progress and made it public so others can clone it, learn from it, and make it their
own. If you build something cool with it, Iâ€™d love to hear about it!

